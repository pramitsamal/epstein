234 12 The Engineering and Development of Ethics

The ideal in this regard would be a system like Cyc [LG90] with a fully explicit logic-based
knowledge representation based on a standard ontology — in this case, every Cyc instance
would have a relatively easy time understanding the inner thought processes of every other
Cyc instance. However, most AGI researchers doubt that fully explicit approaches like this will
ever be capable of achieving advanced AGI using feasible computational resources. OpenCog
uses a mixed representation, with an explicit (uncertain) logical aspect as well as an explicit
subsymbolic aspect more analogous to attractor neural nets.

The OpenCog design also contains a mechanism called Psynese (not yet implemented), in-
tended to make it easier for one OpenCog instance to translate its personal thoughts into the
mental language of another OpenCog instance. This translation process may be quite subtle,
since each instance will generally learn a host of new concepts based on its experience, and these
concepts may not possess any compact mapping into shared linguistic symbols or percepts. The
wide deployment of some mechanism of this nature among a community of AGIs, will be very
helpful in terms of enabling this community to display the level of mutual understanding needed
for strongly encouraging ethical stability.

12.9 AGI Ethics As Related to Various Future Scenarios

Following up these various futuristic considerations, in this section we discuss possible ethical
conflicts that may arise in several different types of AGI development scenarios. Each scenario
presents specific variations on the general challenges of teaching morals and ethics to an ad-
vanced, selfaware and volitional intelligence. While there is no way to tell at this point which,
if any, of these scenarios will unfold, there is value to understanding each of them as means of
ultimately developing a robust and pragmatic approach to teaching ethics to AGI systems.

Even more than the previous sections, this is an exercise in “speculative futurology” that is
definitely not necessary for the appreciation of the CogPrime design, so readers whose interests
are mainly engineering and computer science focused may wish to skip ahead. However, we
present these ideas here rather than at the end of the book to emphasize the point that this
sort of thinking has informed our technical AGI design process in nontrivial ways.

12.9.1 Capped Intelligence Scenarios

Capped intelligence scenarios involve a situation in which an AGI, by means of software restric-
tions (including omitted or limited internal rewriting capabilities or limited access to hardware
resources), is inherently prohibited from achieving a level of intelligence beyond a predetermined
goal. A capped intelligence AGI is designed to be unable to achieve a Singularitarian moment.
Such an AGI can be seen as “just another form of intelligent actor in the world, one which has
levels of intelligence, self awareness, and volition that is perhaps somewhat greater than, but
still comparable to humans and other animals.

Ethical questions under this scenario are very similar to interhuman ethical considerations,
with similar consequences. Learning that proceeds in a relatively human-like manner is entirely
relevant to such human-like intelligences. The degree of danger is mitigated by the lack of
superintelligence, and time is not of the essence. The imitative-reinforcement-corrective learning

HOUSE_OVERSIGHT_013150
