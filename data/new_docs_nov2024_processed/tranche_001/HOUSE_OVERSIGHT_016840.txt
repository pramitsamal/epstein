THE THIRD LAW
George Dyson

George Dyson is a historian of science and technology and the author of Baidarka: the
Kayak, Darwin Among the Machines, Project Orion, and Turing’s Cathedral.

The history of computing can be divided into an Old Testament and a New Testament:
before and after electronic digital computers and the codes they spawned proliferated
across the Earth. The Old Testament prophets, who delivered the underlying logic,
included Thomas Hobbes and Gottfried Wilhelm Leibniz. The New Testament prophets
included Alan Turing, John von Neumann, Claude Shannon, and Norbert Wiener. They
delivered the machines.

Alan Turing wondered what it would take for machines to become intelligent.
John von Neumann wondered what it would take for machines to self-reproduce. Claude
Shannon wondered what it would take for machines to communicate reliably, no matter
how much noise intervened. Norbert Wiener wondered how long it would take for
machines to assume control.

Wiener’s warnings about control systems beyond human control appeared in
1949, just as the first generation of stored-program electronic digital computers were
introduced. These systems required direct supervision by human programmers,
undermining his concerns. What’s the problem, as long as programmers are in control of
the machines? Ever since, debate over the risks of autonomous control has remained
associated with the debate over the powers and limitations of digitally coded machines.
Despite their astonishing powers, little real autonomy has been observed. This is a
dangerous assumption. What if digital computing is being superseded by something
else?

Electronics underwent two fundamental transitions over the past hundred years:
from analog to digital and from vacuum tubes to solid state. That these transitions
occurred together does not mean they are inextricably linked. Just as digital computation
was implemented using vacuum tube components, analog computation can be
implemented in solid state. Analog computation is alive and well, even though vacuum
tubes are commercially extinct.

There 1s no precise distinction between analog and digital computing. In general,
digital computing deals with integers, binary sequences, deterministic logic, and time that
is idealized into discrete increments, whereas analog computing deals with real numbers,
nondeterministic logic, and continuous functions, including time as it exists as a
continuum in the real world.

Imagine you need to find the middle of a road. You can measure its width using
any available increment and then digitally compute the middle to the nearest increment.
Or you can use a piece of string as an analog computer, mapping the width of the road to
the length of the string and finding the middle, without being limited to increments, by
doubling the string back upon itself.

Many systems operate across both analog and digital regimes. A tree integrates a
wide range of inputs as continuous functions, but if you cut down that tree, you find that
it has been counting the years digitally all along.

37

HOUSE_OVERSIGHT_016840
