268 13 Local, Global and Glocal Knowledge Representation

13.6.3 Glocal Hopfield Networks

The ideas in the previous section suggest that, if one wishes to construct an AGI, it is worth
seriously considering using a memory with some sort of glocal structure. One research direction
that follows naturally from this notion is “glocal neural networks.” In order to explore the nature
of glocal neural networks in a relatively simple and tractable setting, we have formalized and
implemented simple examples of “glocal Hopfield networks”: palimpsest Hopfield nets with the
addition of neurons representing localized memories. While these specific networks are not used
in CogPrime, they are quite similar to the ECAN networks that are used in CogPrime and
described in Chapter 23 of Part 2.

Essentially, we augment the standard Hopfield net architecture by adding a set of “key
neurons.” These are a small percentage of the neurons in the network, and are intended to be
roughly equinumerous to the number of memories the network is supposed to store. When the
Hopfield net converges to an attractor A, then new links are created between the neurons that
are active in A, and one of the key neurons. Which key neuron is chosen? The one that, when
it is stimulated, gives rise to an attractor pattern maximally similar to A.

The ultimate result of this is that, in addition to the distributed memory of attractors in the
Hopfield net, one has a set of key neurons that in effect index the attractors. Each attractor
corresponds to a single key neuron. In the glocal memory model, the key neurons are the keys
and the Hopfield net attractors are the maps.

This algorithm has been tested in sparse Hopfield nets, using both standard Hopfield net
learning rules and Storkey’s modified palimpsest learning rule [SV99], which provides greater
memory capacity in a continuous learning context. The use of key neurons turns out to slightly
increase Hopfield net memory capacity, but this isn’t the main point. The main point is that
one now has a local representation of each global memory, so that if one wants to create a
link between the memory and something else, it’s extremely easy to do so — one just needs
to link to the corresponding key neuron. Or, rather, one of the corresponding key neurons:
depending on how many key neurons are allocated, one might end up with a number of key
neurons corresponding to each memory, not just one.

In order to transform a palimpsest Hopfield net into a glocal Hopfield net, the following steps
are taken:

1. Add a fixed number of “key neurons” to the network (removing other random neurons to
keep the total number of neurons constant)

2. When the network reaches an attractor, create links from the elements in the attractor to
one of the key neurons

3. The key neuron chosen for the previous step is the one that most closely matches the current
attractor (which may be determined in several ways, to be discussed below)

4, To avoid the increase of the number of links in the network, when new links are created in
Step 2, other key-neuron links are then deleted (several approaches may be taken here, but
the simplest is to remove the key-neuron links with the lowest-absolute-value weights)

In the simple implementation of the above steps that we implemented, and described in
[GPI 10], Step 3 is carried out simply by comparing the weights of a key neuron’s links to the
nodes in an attractor. A more sophisticated approach would be to select the key neuron with
the highest activation during the transient interval immediately prior to convergence to the
attractor.

HOUSE_OVERSIGHT_013184
