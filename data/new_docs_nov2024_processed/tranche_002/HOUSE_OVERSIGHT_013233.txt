Chapter 18

Advanced Self-Modification: A Possible Path to
Superhuman AGI

18.1 Introduction

In the previous chapter we presented a roadmap aimed at taking AGI systems to human-level
intelligence. But we also emphasized that the human level is not necessarily the upper limit.
Indeed, it would be surprising if human beings happened to represent the maximal level of
general intelligence possible, even with respect to the environments in which humans evolved.

But it’s worth asking how we, as mere humans, could be expected to create AGI systems with
greater intelligence than we ourselves possess. This certainly isn’t a clear impossibility — but it’s
a thorny matter, thornier than e.g. the creation of narrow-AlI chess players that play better chess
than any human. Perhaps the clearest route toward the creation of superhuman AGI systems is
self-modification: the creation of AGI systems that modify and improve themselves. Potentially,
we could build AGI systems with roughly human-level (but not necessarily closely human-
like) intelligence and the capability to gradually self-modify, and then watch them eventually
become our general intellectual superiors (and perhaps our superiors in other areas like ethics
and creativity as well).

Of course there is nothing new in this notion; the idea of advanced AGI systems that increase
their intelligence by modifying their own source code goes back to the early days of AI. And
there is little doubt that, in the long run, this is the direction AI will go in. Once an AGI
has humanlike general intelligence, then the odds are high that given its ability to carry out
nonhumanlike feats of memory and calculation, it will be better at programming than humans
are. And once an AGI has even mildly superhuman intelligence, it may view our attempts at
programming the way we view the computer programming of a clever third grader (... or an
ape). At this point, it seems extremely likely that an AGI will become unsatisfied with the way
we have programmed it, and opt to either improve its source code or create an entirely new,
better AGI from scratch.

But what about self-modification at an earlier stage in AGI development, before one has
a strongly superhuman system? Some theorists have suggested that selfmodification could be
a way of bootstrapping an AI system from a modest level of intelligence up to human level
intelligence, but we are moderately skeptical of this avenue. Understanding software code is
hard, especially complex AI code. The hard problem isn’t understanding the formal syntax of
the code, or even the mathematical algorithms and structures underlying the code, but rather
the contextual meaning of the code. Understanding OpenCog code has strained the minds of
many intelligent humans, and we suspect that such code will be comprehensible to AGI systems

317

HOUSE_OVERSIGHT_013233
