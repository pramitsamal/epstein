those trade-offs: you give up a little versatility, a little humanity, and get a crowd-
pleasing show). Watson is not good company, in spite of misleading ads from IBM that
suggest a general conversational ability, and turning Watson into a plausibly
multidimensional agent would be like turning a hand calculator into Watson. Watson
could be a useful core faculty for such an agent, but more like a cerebellum or an
amygdala than a mind—at best, a special-purpose subsystem that could play a big
supporting role, but not remotely up to the task of framing purposes and plans and
building insightfully on its conversational experiences.

Why would we want to create a thinking, creative agent out of Watson? Perhaps
Turing’s brilliant idea of an operational test has lured us into a trap: the quest to create at
least the illusion of a real person behind the screen, bridging the “uncanny valley.” The
danger, here, is that ever since Turing posed his challenge—which was, after all, a
challenge to fool the judges—AI creators have attempted to paper over the valley with
cutesy humanoid touches, Disneyfication effects that will enchant and disarm the
uninitiated. Weizenbaum’s ELIZA was the pioneer example of such superficial illusion-
making, and it was his dismay at the ease with which his laughably simple and shallow
program could persuade people they were having a serious heart-to-heart conversation
that first sent him on his mission.

He was right to be worried. If there is one thing we have learned from the
restricted Turing Test competitions for the Loebner Prize, it is that even very intelligent
people who aren’t tuned in to the possibilities and shortcuts of computer programming
are readily taken in by simple tricks. The attitudes of people in AI toward these methods
of dissembling at the “user interface” have ranged from contempt to celebration, with a
general appreciation that the tricks are not deep but can be potent. One shift in attitude
that would be very welcome is a candid acknowledgment that humanoid embellishments
are false advertising—something to condemn, not applaud.

How could that be accomplished? Once we recognize that people are starting to
make life-or-death decisions largely on the basis of “advice” from AI systems whose
inner operations are unfathomable in practice, we can see a good reason why those who
in any way encourage people to put more trust in these systems than they warrant should
be held morally and legally accountable. AI systems are very powerful tools—so
powerful that even experts will have good reason not to trust their own judgment over the
“Judgments” delivered by their tools. But then, if these tool users are going to benefit,
financially or otherwise, from driving these tools through ferra incognita, they need to
make sure they know how to do this responsibly, with maximum control and justification.
Licensing and bonding operators, just as we license pharmacists (and crane operators!)
and other specialists whose errors and misjudgments can have dire consequences, can,
with pressure from insurance companies and other underwriters, oblige creators of AI
systems to go to extraordinary lengths to search for and reveal weaknesses and gaps in
their products, and to train those entitled to operate them.

One can imagine a sort of inverted Turing Test in which the judge is on trial; until
he or she can spot the weaknesses, the overstepped boundaries, the gaps in a system, no
license to operate will be issued. The mental training required to achieve certification as
a judge will be demanding. The urge to adopt the intentional stance, our normal tactic
whenever we encounter what seems to be an intelligent agent, is almost overpoweringly
strong. Indeed, the capacity to resist the allure of treating an apparent person as a person

46

HOUSE_OVERSIGHT_016266
