inadequate for describing the mechanisms underlying biological systems, and so he
missed out on how similar mechanisms might eventually be embodied in technological
computational systems—as now they have been. Today’s dominant technologies were
developed in the world of Turing and von Neumann, rather than the world of Wiener.

In the first industrial revolution, energy from a steam engine or a water wheel was
used by human workers to replace their own energy. Instead of being a source of energy
for physical work, people became modulators of how a large source of energy was used.
But because steam engines and water wheels had to be large to be an efficient use of
capital, and because in the 18th century the only technology for spatial distribution of
energy was mechanical and worked only at very short range, many workers needed to be
crowded around the source of energy. Wiener correctly argues that the ability to transmit
energy as electricity caused a second industrial revolution. Now the source of energy
could be distant from where it was used, and from the beginning of the 20th century,
manufacturing could be much more dispersed as electrical-distribution grids were built.

Wiener then argues that a further new technology, that of the nascent
computational machines of his time, will provide yet another revolution. The machines
he talks about seem to be both analog and (perhaps) digital in nature; and he points out, in
The Human Use of Human Beings, that since they will be able to make decisions, both
blue-collar and white-collar workers may be reduced to being mere cogs in a much bigger
machine. He fears that humans might use and abuse one another through organizational
structures that this capability will encourage. We have certainly seen this play out in the
last sixty years, and that disruption 1s far from over.

However, his physics-based view of computation protected him from realizing
just how bad things might get. He saw machines’ ability to communicate as providing a
new and more inhuman way of exerting command and control. He missed that within a
few decades computation systems would become more like biological systems, and it
seems, from his descriptions in chapter 10 of his own work on modeling some aspects of
biology, that he woefully underappreciated the many orders of magnitude of further
complexity of biology over physics. We are in a much more complex situation today
than he foresaw, and I am worried that it is much more pernicious than even his worst
imagined fears.

In the 1960s, computation became firmly based on the foundations set out by
Turing and von Neumann, and it was digital computation, based on the idea of finite
alphabets which they both used. An arbitrarily long sequence, or string, formed by
characters from a finite alphabet, can be encoded as a unique integer. As with Turing
Machines themselves, the formalism for computation became that of computing an
integer-valued function of a single integer-valued input.

Turing and von Neumann both died in the 1950s and at that time this is how they
saw computation. Neither foresaw the exponential increase in computing capability that
Moore’s Law would bring—nor how pervasive computing machinery would become.
Nor did they foresee two developments in our modeling of computation, each of which
poses a great threat to human society.

The first is rooted in the abstractions they adopted. In the fifty-year, Moore’s
Law-fueled race to produce software that could exploit the doubling of computer
capability every two years, the typical care and certification of engineering disciplines
was thrown by the wayside. Software engineering was fast and prone to failures. This

52

HOUSE_OVERSIGHT_016855
