10 1 Introduction

This ties in with the ideas of many other thinkers, including Jeff Hawkins’ “memory/ predic-
tion” theory [B06], and it also speaks directly to the formal characterization of intelligence
presented in Chapter 7: general intelligence as “the ability to achieve complex goals in complex
environments.”

Naturally the goals involved in the above phrase may be explicit or implicit to the intelligent
agent, and they may shift over time as the agent develops.

Perception is taken to mean pattern recognition: the recognition of (novel or familiar) pat-
terns in the environment or in the system itself. Memory is the storage of already-recognized
patterns, enabling recollection or regeneration of these patterns as needed. Action is the for-
mation of patterns in the body and world. Prediction is the utilization of temporal patterns to
guess what perceptions will be seen in the future, and what actions will achieve what effects in
the future — in essence, prediction consists of temporal pattern recognition, plus the (implicit
or explicit) assumption that the universe possesses a habitual tendency according to which
previously observed patterns continue to apply.

1.7.1 Memory and Cognition in CogPrime

Each of these five concepts has a lot of depth to it, and we won’t say too much about them in
this brief introductory overview; but we will take a little time to say something about memory
in particular.

As we'll see in Chapter 7, one of the things that the mathematical theory of general intelli-
gence makes clear is that, if you assume your AI system has a huge amount of computational
resources, then creating general intelligence is not a big trick. Given enough computing power,
a very brief and simple program can achieve any computable goal in any computable environ-
ment, quite effectively. Marcus Hutter’s AT XI design [Hut05] gives one way of doing this,
backed up by rigorous mathematics. Put informally, what this means is: the problem of AGI is
really a problem of coping with inadequate compute resources, just as the problem of natural
intelligence is really a problem of coping with inadequate energetic resources.

One of the key ideas underlying CogPrime is a principle called cognitive synergy, which
explains how real-world minds achieve general intelligence using limited resources, by appropri-
ately organizing and utilizing their memories.

This principle says that there are many different kinds of memory in the mind: sensory,
episodic, procedural, declarative, attentional, intentional. Each of them has certain learning
processes associated with it; for example, reasoning is associated with declarative memory.
Synergy arises here in the way the learning processes associated with each kind of memory have
got to help each other out when they get stuck, rather than working at cross-purposes.

Cognitive synergy is a fundamental principle of general intelligence — it doesn’t tend to play
a central role when you’re building narrow-AI systems.

In the CogPrime approach all the different kinds of memory are linked together in a single
meta-representation, a sort of combined semantic/neural network called the AtomSpace. It
represents everything from perceptions and actions to abstract relationships and concepts and
even a system’s model of itself and others. When specialized representations are used for other
types of knowledge (e.g. program trees for procedural knowledge, spatiotemporal hierarchies
for perceptual knowledge) then the knowledge stored outside the AtomSpace is represented via

HOUSE_OVERSIGHT_012926
