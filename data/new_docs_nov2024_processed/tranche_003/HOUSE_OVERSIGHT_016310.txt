How does one test for thinking? By the Turing Test? Unfortunately, that requires
a thinking judge. One might imagine a vast collaborative project on the Internet, where
an AI hones its thinking abilities in conversations with human judges and becomes an
AGI. But that assumes, among other things, that the longer the judge is unsure whether
the program is a person, the closer it is to being a person. There is no reason to expect
that.

And how does one test for disobedience? Imagine Disobedience as a compulsory
school subject, with daily disobedience lessons and a disobedience test at the end of term.
(Presumably with extra credit for not turning up for any of that.) This is paradoxical.

So, despite its usefulness in other applications, the programming technique of
defining a testable objective and training the program to meet it will have to be dropped.
Indeed, I expect that any testing in the process of creating an AGI risks being
counterproductive, even immoral, just as in the education of humans. I share Turing’s
supposition that we’ll know an AGI when we see one, but this partial ability to recognize
success won’t help in creating the successful program.

In the broadest sense, a person’s quest for understanding is indeed a search
problem, in an abstract space of ideas far too large to be searched exhaustively. But there
is no predetermined objective of this search. There is, as Popper put it, no criterion of
truth, nor of probable truth, especially in regard to explanatory knowledge. Objectives
are ideas like any others—created as part of the search and continually modified and
improved. So inventing ways of disabling the program’s access to most of the space of
ideas won’t help—whether that disability is inflicted with the thumbscrew and stake or a
mental straitjacket. To an AGI, the whole space of ideas must be open. It should not be
knowable in advance what ideas the program can never contemplate. And the ideas that
the program does contemplate must be chosen by the program itself, using methods,
criteria, and objectives that are also the program’s own. Its choices, like an AI’s, will be
hard to predict without running it (we lose no generality by assuming that the program is
deterministic; an AGI using a random generator would remain an AGI if the generator
were replaced by a pseudo-random one), but it will have the additional property that there
is no way of proving, from its initial state, what it won ’t eventually think, short of
running it.

The evolution of our ancestors is the only known case of thought starting up
anywhere in the universe. As I have described, something went horribly wrong, and
there was no immediate explosion of innovation: Creativity was diverted into something
else. Yet not into transforming the planet into paper clips (pace Nick Bostrom). Rather,
as we should also expect if an AGI project gets that far and fails, perverted creativity was
unable to solve unexpected problems. This caused stasis and worse, thus tragically
delaying the transformation of anything into anything. But the Enlightenment has
happened since then. We know better now.

90

HOUSE_OVERSIGHT_016310
