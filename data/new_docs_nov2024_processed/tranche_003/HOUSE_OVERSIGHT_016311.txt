Tom Griffiths’ approach to the AI issue of “value alignment’ —the study of how,
exactly, we can keep the latest of our serial models of Al from turning the planet into
paper clips—is human-centered;, i.e., that of a cognitive scientist, which is what he is.
The key to machine learning, he believes, is, necessarily, human learning, which he
studies at Princeton using mathematical and computational tools.

Tom once remarked to me that “one of the mysteries of human intelligence is that
we're able to do so much with so little.”” Like machines, human beings use algorithms to
make decisions or solve problems; the remarkable difference lies in the human brain’s
overall level of success despite the comparative limits on computational resources.

The efficacy of human algorithms springs from what AI researchers refer to as
“bounded optimality.” As psychologist Daniel Kahneman has notably pointed out,
human beings are rational only up to a point. If you were perfectly rational, you would
risk dropping dead before making an important decision—whom to hire, whom to marry,
and so on—depending on the number of options available for your review.

“With all of the successes of Al over the last few years, we’ve got good models of
things like images and text, but what we’re missing are good models of people,” Tom
says. “Human beings are still the best example we have of thinking machines. By
identifying the quantity and the nature of the preconceptions that inform human cognition
we can lay the groundwork for bringing computers even closer to human performance.”

91

HOUSE_OVERSIGHT_016311
