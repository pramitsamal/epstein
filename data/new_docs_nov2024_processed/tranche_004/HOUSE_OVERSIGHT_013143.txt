12.6 The Ethical Treatment of AGIs 227

Law Principle

Zeroth A robot must not merely act in the interests of individual
humans, but of all humanity.

First A robot may not injure a human being or, through inaction,
allow a human being to come to harm.

Second A robot must obey orders given it by human beings except
where such orders would conflict with the First Law.

Third A robot must protect its own existence as long as such pro-

tection does not conflict with the First or Second Law.

Table 12.7: Asimov’s Three Laws of Robotics

indefinitely. The second law also casts humanity in the role of slavemaster, a situation which
history shows leads to moral degradation.

Unlike Asimov in his fiction, we consider it critical that AGI ethics be construed to encompass
both “human ethicalness to AGIs” and “AGI ethicalness to humans.” The multiple-imperatives
approach we explore here suggests that, in many contexts, these two aspects of AGI ethics may
be best addressed jointly.

The issue of ethicalness to AGIs has not been entirely avoided in the literature, however.
Wallach [WA10] considers it in some detail; and Thomas Metzinger (in the final chapter of
[Met04]) has argued that creating AGI is in itself an unethical pursuit, because early-stage
AGIs will inevitably be badly-built, so that their subjective experiences will quite possibly be
extremely unpleasant in ways we can’t understand or predict. Our view is that this is a serious
concern, which however is most probably avoidable via appropriate AGI designs and teaching
methodologies. To address Metzinger’s concern one must create AGIs that, right from the start,
are adept at communicating their states of minds in a way we can understand both analytically
and empathically. There is no reason to believe this is impossible, but, it certainly constitutes
a large constraint on the class of AGI architectures to be pursued. On the other hand, there is
an argument that this sort of AGI architecture will also be the easiest one to create, because it
will be the easiest kind for humans to instruct.

And this leads on to a topic that is central to our work with CogPrime in several respects:
imitative learning. The way humans achieve empathic interconnection is in large part via being
wired for imitation. When we perceive another human carrying out an action, mirror neuron
systems in our brains respond in many cases as if we ourselves were carrying out the action (see
[Per70, Per81] and Appendix ??). This obviously primes us for carrying out the same actions
ourselves later on: i.e., the capability and inclination for imitative learning is explicitly encoded
in our brains. Given the efficiency of imitative learning as a means of acquiring knowledge, it
seems extremely likely that any successful early-stage AGIs are going to utilize this methodology
as well. CogPrime utilizes imitative learning as a key aspect. Thus, at least some current AGI
work is occurring in a manner that would plausibly circumvent Metzinger’s ethical complaint.

Obviously, the use of imitative learning in AGI systems has further specific implications for
AGI ethics. It means that (much as in the case of interaction with other humans) what we do
to and around AGIs has direct implications for their behavior and their well-being. We suggest
that among early-stage AGI’s capable of imitative learning, one of the most likely sources
for AGI misbehavior is imitative learning of antisocial behavior from human companions. “Do
as I say, not as I do” may have even more dire consequences as an approach to AGI ethics
pedagogy than the already serious repercussions it has when teaching humans. And there may
well be considerable subtlety to such phenomena; behaviors that are violent or oppressive to

HOUSE_OVERSIGHT_013143
