superintelligence to recursively improve its superintelligence, from the instant it is turned
on we will be powerless to stop it.

But these scenarios are based on a confusion of intelligence with motivation—of
beliefs with desires, inferences with goals, the computation elucidated by Turing and the
control elucidated by Wiener. Even if we did invent superhumanly intelligent robots,
why would they want to enslave their masters or take over the world? Intelligence is the
ability to deploy novel means to attain a goal. But the goals are extraneous to the
intelligence: Being smart is not the same as wanting something. It just so happens that
the intelligence in Homo sapiens is a product of Darwinian natural selection, an
inherently competitive process. In the brains of that species, reasoning comes bundled
with goals such as dominating rivals and amassing resources. But it’s a mistake to
confuse a circuit in the limbic brain of a certain species of primate with the very nature of
intelligence. There is no law of complex systems that says that intelligent agents must
turn into ruthless megalomaniacs.

A second misconception is to think of intelligence as a boundless continuum of
potency, a miraculous elixir with the power to solve any problem, attain any goal. The
fallacy leads to nonsensical questions like when an AI will “exceed human-level
intelligence,” and to the image of an “artificial general intelligence” (AGI) with God-like
omniscience and omnipotence. Intelligence is a contraption of gadgets: software modules
that acquire, or are programmed with, knowledge of how to pursue various goals in
various domains. People are equipped to find food, win friends and influence people,
charm prospective mates, bring up children, move around in the world, and pursue other
human obsessions and pastimes. Computers may be programmed to take on some of
these problems (like recognizing faces), not to bother with others (like charming mates),
and to take on still other problems that humans can’t solve (like simulating the climate or
sorting millions of accounting records). The problems are different, and the kinds of
knowledge needed to solve them are different.

But instead of acknowledging the centrality of knowledge to intelligence, the
dystopian scenarios confuse an artificial general intelligence of the future with Laplace’s
demon, the mythical being that knows the location and momentum of every particle in
the universe and feeds them into equations for physical laws to calculate the state of
everything at any time in the future. For many reasons, Laplace’s demon will never be
implemented in silicon. A real-life intelligent system has to acquire information about the
messy world of objects and people by engaging with it one domain at a time, the cycle
being governed by the pace at which events unfold in the physical world. That’s one of
the reasons that understanding does not obey Moore’s Law: Knowledge is acquired by
formulating explanations and testing them against reality, not by running an algorithm
faster and faster. Devouring the information on the Internet will not confer omniscience
either: Big Data is still finite data, and the universe of knowledge is infinite.

A third reason to be skeptical of a sudden AI takeover is that it takes too seriously
the inflationary phase in the AI hype cycle in which we are living today. Despite the
progress in machine learning, particularly multilayered artificial neural networks, current
AI systems are nowhere near achieving general intelligence (if that concept is even
coherent). Instead, they are restricted to problems that consist of mapping well-defined
inputs to well-defined outputs in domains where gargantuan training sets are available, in
which the metric for success is immediate and precise, in which the environment doesn’t

81

HOUSE_OVERSIGHT_016301
