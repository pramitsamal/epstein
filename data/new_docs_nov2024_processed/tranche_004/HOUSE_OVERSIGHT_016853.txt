THE INHUMAN MESS OUR MACHINES HAVE GOTTEN US INTO
Rodney Brooks

Rodney Brooks is a computer scientist; Panasonic Professor of Robotics, emeritus, MIT;
former director, MIT Computer Science Lab; and founder, chairman, and CTO of
Rethink Robotics. He is the author of Flesh and Machines.

Mathematicians and scientists are often limited in how they see the big picture, beyond
their particular field, by the tools and metaphors they use in their work. Norbert Wiener
is no exception, and I might guess that neither am I.

When he wrote The Human Use of Human Beings, Wiener was straddling the end
of the era of understanding machines and animals simply as physical processes and the
beginning of our current era of understanding machines and animals as computational
processes. I suspect there will be future eras whose tools will look as distinct from the
tools of the two eras Wiener straddled as those tools did from each other.

Wiener was a giant of the earlier era and built on the tools developed since the
time of Newton and Leibniz to describe and analyze continuous processes in the physical
world. In 1948 he published Cybernetics, a word he coined to describe the science of
communication and control in both machines and animals. Today we would refer to the
ideas in this book as control theory, an indispensable discipline for the design and
analysis of physical machines, while mostly neglecting Wiener’s claims about the science
of communication. Wiener’s innovations were largely driven by his work during the
Second World War on mechanisms to aim and fire anti-aircraft guns. He brought
mathematical rigor to the design of the sorts of technology whose design processes had
been largely heuristic in nature: from the Roman waterworks through Watt’s steam
engine to the early development of automobiles.

One can imagine a different contingent version of our intellectual and
technological history had Alan Turing and John von Neumann, both of whom made
major contributions to the foundations of computing, not appeared on the scene. Turing
contributed a fundamental model of computation—now known as a Turing Machine—in
his paper “On Computable Numbers with an Application to the Entscheidungsproblem,”
written and revised in 1936 and published in 1937. In these machines, a linear tape of
symbols from a finite alphabet encodes the input for a computational problem and also
provides the working space for the computation. A different machine was required for
each separate computational problem; later work by others would show that in one
particular machine, now known as a Universal Turing Machine, an arbitrary set of
computing instructions could be encoded on that same tape.

In the 1940s, von Neumann developed an abstract self-reproducing machine
called a cellular automaton. In this case it occupied a finite subset of an infinite two-
dimensional array of squares each containing a single symbol from a finite alphabet of
twenty-nine distinct symbols—the rest of the infinite array starts out blank. The single
symbols in each square change in lockstep, based on a complex but finite rule about the
current symbol in that square and its immediate neighbors. Under the complex rule that
von Neumann developed, most of the symbols in most of the squares stay the same and a
few change at each step. So when one looks at the non-blank squares, it appears that

50

HOUSE_OVERSIGHT_016853
