search exhaustively. Every improvement in chess-playing Als, between Alan Turing’s
first design for one in 1948 and today’s, has been brought about by ingeniously confining
the program’s attention (or making it confine its attention) ever more narrowly to
branches likely to lead to that immutable goal. Then those branches are evaluated
according to that goal.

That is a good approach to developing an AI with a fixed goal under fixed
constraints. But if an AGI worked like that, the evaluation of each branch would have to
constitute a prospective reward or threatened punishment. And that is diametrically the
wrong approach if we’re seeking a befter goal under unknown constraints—which is the
capability of an AGI. An AGI is certainly capable of learning to win at chess—but also
of choosing not to. Or deciding in mid-game to go for the most interesting continuation
instead of a winning one. Or inventing anew game. A mere AI is incapable of having
any such ideas, because the capacity for considering them has been designed out of its
constitution. That disability is the very means by which it plays chess.

An AGI is capable of enjoying chess, and of improving at it because it enjoys
playing. Or of trying to win by causing an amusing configuration of pieces, as grand
masters occasionally do. Or of adapting notions from its other interests to chess. In other
words, it learns and plays chess by thinking some of the very thoughts that are forbidden
to chess-playing Als.

An AGT is also capable of refusing to display any such capability. And then, if
threatened with punishment, of complying, or rebelling. Daniel Dennett, in his essay for
this volume, suggests that punishing an AGI is impossible:

[L]ike Superman, they are too invulnerable to be able to make a credible
promise. ... What would be the penalty for promise- breaking? Being
locked in a cell or, more plausibly, dismantled?. . . The very ease of
digital recording and transmitting—the breakthrough that permits
software and data to be, in effect, immortal—removes robots from the
world of the vulnerable. . . .

But this is not so. Digital immortality (which is on the horizon for humans, too,
perhaps sooner than AGI) does not confer this sort of invulnerability. Making a
(running) copy of oneself entails sharing one’s possessions with it somehow—including
the hardware on which the copy runs—so making such a copy is very costly for the AGI.
Similarly, courts could, for instance, impose fines on a criminal AGI which would
diminish its access to physical resources, much as they do for humans. Making a backup
copy to evade the consequences of one’s crimes is similar to what a gangster boss does
when he sends minions to commit crimes and take the fall if caught: Society has
developed legal mechanisms for coping with this.

But anyway, the idea that it is primarily for fear of punishment that we obey the
law and keep promises effectively denies that we are moral agents. Our society could not
work if that were so. No doubt there will be AGI criminals and enemies of civilization,
just as there are human ones. But there is no reason to suppose that an AGI created in a
society consisting primarily of decent citizens, and raised without what William Blake
called “mind-forg’d manacles,” will in general impose such manacles on itself (1.e.,
become irrational) and/or choose to be an enemy of civilization.

88

HOUSE_OVERSIGHT_016891
