information required to gain knowledge of an event is dependent upon the
probability of its occurrence. log2(0.5) = 1 is the maximal entropy when modeling the
equilibrium entropy of an independent random 0,1, (heads or tails) series of
informational states as might result from flipping a fair coin a large number of times.
This value would be maximal when the coin was fair, p(heads, tails) = 0.5, and the
entropy would be 2(number of allowed states)x0.5(probability of occupying each
state)x/ogio (0.5) = 0.693147...or in bits, log2(0.5) = 1.

More generally, if system’s behavior is distributed equally among its possible
states, the Shannon entropy is maximal and equal to the logarithm of the number of
defined states, for example, log2 (2) = 1. Shannon’s classical equation about
information content says the amount of information, / = -p /og2 p, measured in bits.
The minus sign in this reciprocal relation indicates that the information content of
data, /, goes up as the probability of occurrence of the observed data, p, goes
down. Since soon we will be talking about brains and their various styles of
information encoded content as well as its transmission, we note the other famous
Shannon theorem dealing with limits on the channel capacity, C, for information
transport is C = Wlog2(1+S/N) where W is bandwidth, the range of frequencies
available for information transport, S is the strength of the signal and WN is the
strength of the noise. Recall that the /og2(7) = 0 so only the signal-to-noise ratio,
S/N contributes to the value of the product of the multiplication by bandwidth, W.
Transparent clinical examples come from studies of the perceptual and cognitive
decline in normal geriatric patients in which the range of aural frequencies (W)
heard without augmentation decreases with age as does the frequency range (W)
observed in their resting brain waves. The inattentiveness of the obsessively
worried ruminator can be used as an example of brain channel capacity being
reduced by the amount of on going head noise, an increase N, which, of course,
reduces the value of S/N and therefore C.

Measures of the informational complexity of systems in motion, in contrast
with the information content of a static equilibrium state, are of dynamical entropy.
Dynamical entropy is often called H, in contrast with thermodynamic and/or

informational entropy, S. One can begin with a representational image of the

78

HOUSE_OVERSIGHT_013578
