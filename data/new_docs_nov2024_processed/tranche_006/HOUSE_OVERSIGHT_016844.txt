Dan Dennett is the philosopher of choice in the AI community. He is perhaps best
known in cognitive science for his concept of intentional systems and his model of human
consciousness, Which sketches a computational architecture for realizing the stream of
consciousness in the massively parallel cerebral cortex. That uncompromising
computationalism has been opposed by philosophers such as John Searle, David
Chalmers, and the late Jerry Fodor, who have protested that the most important aspects
of consciousness—intentionality and subjective qualia—cannot be computed.

Twenty-five years ago, I was visiting Marvin Minsky, one of the original AI
pioneers, and asked him about Dan. “He’s our best current philosopher—the next
Bertrand Russell,” said Marvin, adding that unlike traditional philosophers, Dan was a
student of neuroscience, linguistics, artificial intelligence, computer science, and
psychology: “He’s redefining and reforming the role of the philosopher. Of course, Dan
doesn’t understand my Society-of-Mind theory, but nobody’s perfect.”

Dan’s view of the efforts of AI researchers to create superintelligent AIs is
relentlessly levelheaded. What, me worry? In this essay, he reminds us that Als, above
all, should be regarded—and treated—as tools and not as humanoid colleagues.

He has been interested in information theory since his graduate school days at
Oxford. In fact, he told me that early in his career he was keenly interested in writing a
book about Wiener’s cybernetic ideas. As a thinker who embraces the scientific method,
one of his charms is his willingness to be wrong. Of a recent piece entitled “What Is
Information?” he has announced, “TI stand by it, but it’s under revision. I’m already
moving beyond it and realizing there’s a better way of tackling some of these issues.” He
will most likely remain cool and collected on the subject of AI research, although he has
acknowledged, often, that his own ideas evolve—as anyone ’s ideas should.

41

HOUSE_OVERSIGHT_016844
