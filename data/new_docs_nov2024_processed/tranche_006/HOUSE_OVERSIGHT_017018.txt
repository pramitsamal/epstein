II.3B. Generation of historical n-grams corpora

To generate a particular historical n-grams corpus, a subset of book editions is chosen to serve as the
base corpus. The chosen editions are divided by publication year. For each publication year, total counts
for each n-gram are obtained by summing n-gram counts for each book edition that was published in that
year. In particular, three counts are generated: (1) the total number of times the n-gram appears; (2) the
number of pages on which the n-gram appears; and (3) the number of books in which the n-gram
appears.

We then generate tables showing all three counts for each n-gram, resolved by year. In order to ensure
that n-grams could not be easily used to identify individual text sources, we did not report counts for any
n-grams that appeared fewer than 40 times in the corpus. (As a point of reference, the total number of 1-
grams that appear in the 3.2 million books written in English with highest date accuracy (‘eng-all’, see
below) is 360 billion: a 1-gram that would appear fewer than 40 times occurs at a frequency of the order
of 10° ') As a result, rare spelling and OCR errors were also omitted. Since most n-grams are infrequent,
this also served to dramatically reduce the size of the n-gram tables. Of course, the most robust historical
trends are associated with frequent n-grams, so our ability to discern these trends was not compromised
by this approach.

By dividing the reported counts by the corpus size (measured in either words, pages, or books), it is
possible to determine the normalized frequency with which an n-gram appears in the base corpus. Note
that the different counts can be used for different purposes. The usage frequency of an n-gram,
normalized by the total number of words, reflects both the number of authors using an n-gram, and how
frequently they use it. It can be driven upward markedly by a single author who uses an n-gram very
frequently, for instance in a biography of 'Gottlieb Daimler’ which mentions his name many times. This
latter effect is sometimes undesirable. In such cases, it may be preferable to examine the fraction of
books containing a particular n-gram: texts in different books, which are usually written by different
authors, tend to be more independent.

Eleven corpora were generated, based on eleven different subsets of books. Five of these are English
language corpora, and six are foreign language corpora.

Eng-all

This is derived from a base corpus containing all English language books which pass the filters described
in section 1.

Eng-1M

This is derived from a base corpus containing 1 million English language books which passed the filters
described in section 1. The base corpus is a subset of the Eng-all base corpus.

The sampling was constrained in two ways.

First, the texts were re-sampled so as to exhibit a representative subject distribution. Because digitization
depends on the availability of the physical books (from libraries or publishers), we reasoned that digitized
books may be a biased subset of books as a whole. We therefore re-sampled books so as to ensure that
the diversity of book editions included in the corpus for a given year, as reflected by BISAC subject
codes, reflected the diversity of book editions actually published in that year. We estimated the latter
using our metadata database, which reflects the aggregate of our 100 bibliographic sources and includes
10-fold more book editions than the scanned collection.

Second, the total number of books drawn from any given year was capped at 6174. This has the net
effect of ensuring that the total number of books in the corpus is uniform starting around the year 1883.
This was done to ensure that all books passing the quality filters were included in earlier years. This

10

HOUSE_OVERSIGHT_017018
