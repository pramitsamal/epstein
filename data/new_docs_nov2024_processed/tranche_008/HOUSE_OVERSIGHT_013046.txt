130 7 A Formal Model of Intelligent Agents

about the dynamics of general intelligence, which has been useful in guiding development of the
ECAN component of CogPrime, and we expect will have more general value in future.

Despite the intermittent use of mathematical formalism, the ideas presented in this section
are fairly speculative, and we do not propose them as constituting a well-demonstrated theory
of general intelligence. Rather, we propose them as an interesting way of thinking about general
intelligence, which appears to be consistent with available data, and which has proved inspira-
tional to us in conceiving concrete structures and dynamics for AGI, as manifested for example
in the CogPrime design. Understanding the way of thinking described in these chapters is valu-
able for understanding why the CogPrime design is the way it is, and for relating CogPrime to
other practical and intellectual systems, and extending and improving CogPrime.

7.2 A Simple Formal Agents Model (SRAM)

We now present a formalization of the concept of “intelligent agents” — beginning with a for-
malization of “agents” in general.

Drawing on [Iut05, LM07a], we consider a class of active agents which observe and explore
their environment and also take actions in it, which may affect the environment. Formally,
the agent sends information to the environment by sending symbols from some finite alphabet
called the action space 7; and the environment sends signals to the agent with symbols from
an alphabet called the perception space, denoted P. Agents can also experience rewards, which
lie in the reward space, denoted R, which for each agent is a subset of the rational unit interval.

The agent and environment are understood to take turns sending signals back and forth,
yielding a history of actions, observations and rewards, which may be denoted

a1,0171aA909Pr9...

or else

A,X AQX)...

if a is introduced as a single symbol to denote both an observation and a reward. The
complete interaction history up to and including cycle t is denoted aa1..; and the history before
cycle t is denoted ave, = ax14_1.

The agent is represented as a function 7 which takes the current history as input, and pro-
duces an action as output. Agents need not be deterministic, an agent may for instance induce a
probability distribution over the space of possible actions, conditioned on the current history. In
this case we may characterize the agent by a probability distribution 7(a:|av<¢). Similarly, the
environment may be characterized by a probability distribution (7, lav <,ax). Taken together,
the distributions 7 and y define a probability measure over the space of interaction sequences.

Next, we extend this model in a few ways, intended to make it better reflect the realities of
intelligent computational agents. The first modification is to allow agents to maintain memories
(of finite size), via adding memory actions drawn from a set M into the history of actions,
observations and rewards. The second modification is to introduce the notion of goals.

HOUSE_OVERSIGHT_013046
