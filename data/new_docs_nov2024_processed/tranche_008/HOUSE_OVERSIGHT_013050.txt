134 7 A Formal Model of Intelligent Agents

Context & Procedure — Goal

and considered more formally as holds(C) & ex(P) > h, where h may be an externally specified
goal g; or an internally specified goal h derived as a (possibly uncertain) subgoal of one of more
gi; C is a piece of declarative or episodic knowledge and P is a procedure that the agent can
internally execute to generate a series of actions. ex(P) is the proposition that P is successfully
executed. If C is episodic then holds(C) may be interpreted as the current context (i.e. some
finite slice of the agent’s history) being similar to C; if C is declarative then holds(C) may be
interpreted as the truth value of C' evaluated at the current context. Note that C may refer to
some part of the world quite distant from the agent’s current sensory observations; but it may
still be formally evaluated based on the agent’s history.

In the standard CogPrime notation as introduced formally in Chapter 20 (where indentation
has function-argument syntax similar to that in Python, and relationship types are prepended
to their relata without parentheses), for the case C is declarative this would be written as

PredictiveExtensionallmplication
AND
C
Execution P

G

and in the case C' is episodic one replaces C in this formula with a predicate expressing C’s
similarity to the current context. The semantics of the PredictiveExtensionalInheritance relation
will be discussed below. The Execution relation simply denotes the proposition that procedure
P has been executed.

For the class of SRAM agents who (like CogPrime) use the cognitive schematic to govern
many or all of their actions, a significant fragment of agent intelligence boils down to estimating
the truth values of PredictiveExtensionallmplication relationships. Action selection procedures
can be used, which choose procedures to enact based on which ones are judged most likely
to achieve the current external goals g; in the current context. Rather than enter into the
particularities of action selection or other cognitive architecture issues, we will restrict ourselves
to PLN inference, which in the context of the present agent model is a method for handling
Predictivelmplication in the cognitive schematic.

Consider an agent in a virtual world, such as a virtual dog, one of whose external goals is to
please its owner. Suppose its owner has asked it to find a cat, and it can translate this into a
subgoal “find cat.” If the agent operates according to the cognitive schematic, it will search for
P so that

PredictiveExtensionallmplication
AND
C
Execution P
Evaluation
found
cat

holds.

HOUSE_OVERSIGHT_013050
