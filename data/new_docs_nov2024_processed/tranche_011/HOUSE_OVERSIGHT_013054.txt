138 7 A Formal Model of Intelligent Agents

7.3.3 Pragmatic General Intelligence

The above concept of biased universal intelligence is perfectly adequate for many purposes, but
it is also interesting to explicitly introduce the notion of a goal into the calculation. This allows
us to formally capture the notion presented in [Goe93a] of intelligence as “the ability to achieve
complex goals in complex environments.”

If the agent is acting in environment jz, and is provided with g, corresponding to g at the
start and the end of the time-interval T = {i € (s,...,¢)}, then the expected goal-achievement
of the agent, relative to g, during the interval is the expectation

t
ViiaT = OD T(Ig,s,i))
1=s
where the expectation is taken over all interaction sequences J, ., drawn according to pp. We
then propose

Definition 5 The pragmatic general intelligence of an agent 7, relative to the distribution
vy over environments and the distribution y over goals, is its expected performance with respect
to goals drawn from y in environments drawn from v, over the time-scales natural to the goals;
that is,

HEE GEG,T

(in those cases where this sum is convergent).

This definition formally captures the notion that “intelligence is achieving complex goals in
complex environments,” where “complexity” is gauged by the assumed measures v and 4.

If v is taken to be the universal distribution, and + is defined to weight goals according to
the universal distribution, then pragmatic general intelligence reduces to universal intelligence.

Furthermore, it is clear that a universal algorithmic agent like ATXT [Hut05] would also
have a high pragmatic general intelligence, under fairly broad conditions. As the interaction
history grows longer, the pragmatic general intelligence of AIXI would approach the theoretical
maximum; as AIXI would implicitly infer the relevant distributions via experience. However,
if significant reward discounting is involved, so that near-term rewards are weighted much
higher than long-term rewards, then AIXI might compare very unfavorably in pragmatic general
intelligence, to other agents designed with prior knowledge of v, y and 7 in mind.

The most interesting case to consider is where v and ¥ are taken to embody some particular
bias in a real-world space of environments and goals, and this bias is appropriately reflected
in the internal structure of an intelligent agent. Note that an agent needs not lack universal
intelligence in order to possess pragmatic general intelligence with respect to some non-universal
distribution over goals and environments. However, in general, given limited resources, there
may be a tradeoff between universal intelligence and pragmatic intelligence. Which leads to the
next point: how to encompass resource limitations into the definition.

One might argue that the definition of Pragmatic General Intelligence is already encompassed
by Legg and Hutter’s definition because one may bias the distribution of environments within
the latter by considering different Turing machines underlying the Kolmogorov complexity.
However this is not a general equivalence because the Solomonoff-Levin measure intrinsically

HOUSE_OVERSIGHT_013054
