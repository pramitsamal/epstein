That could be done for twenty-six different possibilities, but it couldn’t be done for ten
thousand. It was just a matter of scaling up the whole system that makes this possible
today. There are maybe five thousand picturable common nouns in English, ten thousand
if you include things like special kinds of plants and beetles which people would
recognize with some frequency. What we did was train our system on 30 million images
of these kinds of things. It’s a big, complicated, messy neural network. The details of
the network probably don’t matter, but it takes about a quadrillion GPU operations to do
the training.

Our system is impressive because it pretty much matches what humans can do. It
has about the same training data humans have—about the same number of images a
human infant would see in the first couple of years of its life. Roughly the same number
of operations have to be done in the learning process, using about the same number of
neurons in at least the first levels of our visual cortex. The details are different; the way
these artificial neurons work has little to do with how the brain’s neurons work. But the
concept is similar, and there’s a certain universality to what’s going on. At the
mathematical level, it’s a composition of a very large number of functions, with certain
continuity properties that let you use calculus methods to incrementally train the system.
Given those attributes, you can end up with something that does the same job human
brains do in physiological recognition.

But does this constitute AI? There are a few basic components. There’s
physiological recognition, there’s voice-to-text, there’s language translation—things
humans manage to do with varying degrees of difficulty. These are essentially some of
the links to how we make machines that are humanlike in what they do. For me, one of
the interesting things has been incorporating those capabilities into a precise symbolic
language to represent the everyday world. We now have a system that can say, “This is a
glass of water.” We can go from a picture of a glass of water to the concept of a glass of
water. Now we have to invent some actual symbolic language to represent those
concepts.

I began by trying to represent mathematical, technical kinds of knowledge and
went on to other kinds of knowledge. We’ve done a pretty good job of representing
objective knowledge in the world. Now the problem is to represent everyday human
discourse in a precise symbolic way—a knowledge-based language intended for
communication between humans and machines, so that humans can read it and machines
can understand it, too. For instance, you might say “X is greater than 5.” That’s a
predicate. You might also say, “I want a piece of chocolate.” That’s also a predicate. It
has an “I want” init. We have to find a precise symbolic representation of the desires we
express in human natural language.

In the late 1600s, Gottfried Leibniz, John Wilkins, and others were concerned
with what they called philosophical languages—that is, complete, universal, symbolic
representations of things in the world. You can look at the philosophical language of
John Wilkins and see how he divided up what was important in the world at the time.
Some aspects of the human condition have been the same since the 1600s. Some are very
different. His section on death and various forms of human suffering was huge; in
today’s ontology, it’s alot smaller. It’s interesting to see how a philosophical language
of today would differ from a philosophical language of the mid-1600s. It’s a measure of
our progress. Many such attempts at formalization have happened over the years. In

186

HOUSE_OVERSIGHT_016406
