8.7 Is Cognitive Synergy Tricky? 157

For instance, the PLN controller could make a list of everyone who has been a regular
visitor, and everyone who has not been, and pose MOSES the task of figuring out a procedure
for distinguishing these two categories. This procedure could then used directly to make the
needed assessment, or else be translated into logical rules to be used within PLN inference. For
example, perhaps MOSES would discover that older males wearing ties tend not to become
regular visitors. If the new playmate is an older male wearing a tie, this is directly applicable.
But if the current playmate is wearing a tuxedo, then PLN may be helpful via reasoning that
even though a tuxedo is not a tie, it’s a similar form of fancy dress — so PLN may extend the
MOSES-learned rule to the present case and infer that the new playmate is not likely to be a
regular visitor.

8.7 Is Cognitive Synergy Tricky?

In this section we use the notion of cognitive synergy to explore a question that arises
frequently in the AGI community: the well-known difficulty of measuring intermediate progress
toward human-level AGI. We explore some potential reasons underlying this, via extending the
notion of cognitive synergy to a more refined notion of tricky cognitive synergy. These ideas
are particularly relevant to the problem of creating a roadmap toward AGI, as we'll explore in
Chapter 17 below.

8.7.1 The Puzzle: Why Is It So Hard to Measure Partial Progress
Toward Human-Level AGI?

It’s not entirely straightforward to create tests to measure the final achievement of human-level
AGI, but there are some fairly obvious candidates here. There’s the Turing Test (fooling judges
into believing you’re human, in a text chat), the video Turing Test, the Robot College Student
test (passing university, via being judged exactly the same way a human student would), etc.
There’s certainly no agreement on which is the most meaningful such goal to strive for, but
there’s broad agreement that a number of goals of this nature basically make sense.

On the other hand, how does one measure whether one is, say, 50 percent of the way to
human-level AGI? Or, say, 75 or 25 percent?

It’s possible to pose many practical tests of incremental progress toward human-level AGI,
with the property that if a proto-AGI system passes the test using a certain sort of architecture
and/or dynamics, then this implies a certain amount of progress toward human-level AGI based
on particular theoretical assumptions about AGI. However, in each case of such a practical test,
it seems intuitively likely to a significant percentage of AGI researchers that there is some way
to game the test via designing a system specifically oriented toward passing that test, and
which doesn’t constitute dramatic progress toward AGI.

Some examples of practical tests of this nature would be

1 This section co-authored with Jared Wigmore

HOUSE_OVERSIGHT_013073
