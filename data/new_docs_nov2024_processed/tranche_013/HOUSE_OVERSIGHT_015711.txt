Mind over Computer 21

hair, or approximately two hundred atoms wide. To match the complexity
of a brain we will need to pack an order of ten million more gates into
a silicon chip. One way to achieve this is to simply shrink the wires, but
when we get down to around ten atoms wide, quantum effects begin to
dominate. Signals in today’s chips involve tens of thousands of electrons.
We normally think of these electrons as a group, but in these tiny circuits
we need to consider the behavior of each individual electron. Problems
arise as this behavior is subject to quantum uncertainty. With only ten
electrons there is a finite probability that none of them will be where you
were expecting them to be. This causes problems for digital logic. You
can't put a ‘1’ ina memory location and be sure when you come to read it
you will get a ‘1’ back. You have to factor in the possibility of error.

Quantum effects can be annoying - requiring us to devise all
manner of error checking hardware — but they can also be helpful.
Richard Feynman proposed using quantum bits, ‘qubits, to perform
computation. Quantum computers can calculate many times faster than
a classical computer because a single bit can represent more than one
piece of information. Enterprising entrepreneurs are making use of this
effect to build the next generation of devices, and you can already buy a
512 qubit computer from a Canadian company called D-Wave.

The biggest problem with building more powerful conventional
chips is their area is reaching the manufacturing limit for economic
viability. Silicon wafers contain random spots of damage and, as a
chip gets larger, the chance it will have one of these spots approaches

certainty. One solution is to use the third dimension and print the logic

HOUSE_OVERSIGHT_015711
