3.4 The General Structure of Cognitive Dynamics: Analysis and Synthesis 49

long periods of time, small changes within the context of the existing self may suffice to allow
the system to control itself intelligently.

Humans can also develop what are known as subselves [Row90]. A subself is a partially
autonomous self-network focused on particular tasks, environments or interactions. It contains
a unique model of the whole organism, and generally has its own set of episodic memories,
consisting of memories of those intervals during which it was the primary dynamic mode con-
trolling the organism. One common example is the creative subself — the subpersonality that
takes over when a creative person launches into the process of creating something. In these
times, a whole different personality sometimes emerges, with a different sort of relationship
to the world. Among other factors, creativity requires a certain open-ness that is not always
productive in an everyday life context, so it’s natural for the selfsystem of a highly creative
person to bifurcate into one self-system for everyday life, and another for the protected context
of creative activity. This sort of phenomenon might emerge naturally in CogPrime systems as
well if they were exposed to appropriate environments and social situations.

Finally, it is interesting to speculate regarding how self may differ in future AI systems as
opposed to in humans. The relative stability we see in human selves may not exist in AI systems
that can self#improve and change more fundamentally and rapidly than humans can. There may
be a situation in which, as soon as a system has understood itself decently, it radically modifies
itself and hence violates its existing self-model. Thus: intelligence without a long-term stable self.
In this case the “attractor-ish” nature of the self holds only over much shorter time scales than
for human minds or human-like minds. But the alternating process of synthesis and analysis
for self-construction is still critical, even though no reasonably stable self-constituting attractor
ever emerges. The psychology of such intelligent systems will almost surely be beyond human
beings’ capacity for comprehension and empathy.

3.4.4.2 Attentional Focus

Finally, we turn to the notion of an “attentional focus” similar to Baars’ [Baa97] notion of a
Global Workspace, which will be reviewed in more detail in Chapter 4: a collection of mental
entities that are, at a given moment, receiving far more than the usual share of an intelligent
system’s computational resources. Due to the amount of attention paid to items in the atten-
tional focus, at any given moment these items are in large part driving the cognitive processes
going on elsewhere in the mind as well - because the cognitive processes acting on the items in
the attentional focus are often involved in other mental items, not in attentional focus, as well
(and sometimes this results in pulling these other items into attentional focus). An intelligent
system must constantly shift its attentional focus from one set of entities to another based on
changes in its environment and based on its own shifting discoveries.

In the human mind, there is a self-reinforcing dynamic pertaining to the collection of entities
in the attentional focus at any given point in time, resulting from the observation that: If A
is in the attentional focus, and A and B have often been associated in the past, then odds
are increased that B will soon be in the attentional focus. This basic observation has been
refined tremendously via a large body of cognitive psychology work; and neurologically it follows
not only from Hebb’s [Heb49] classic work on neural reinforcement learning, but also from
numerous more modern refinements [SB98]. But it implies that two items A and B, if both in
the attentional focus, can reinforce each others’ presence in the attentional focus, hence forming
a kind of conspiracy to keep each other in the limelight. But of course, this kind of dynamic

HOUSE_OVERSIGHT_012965
