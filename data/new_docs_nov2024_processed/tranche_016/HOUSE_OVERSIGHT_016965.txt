that you would not depict this particular, sun-scorched, caterpillar-chewed clover found
outside your house in an atlas. No, you aimed—if you were a genius natural philosopher
like Goethe, Albinus, or Cheselden—to observe nature but then to perfect the object in
question, to abstract it visually to the ideal. Take a skeleton, view it through a camera
lucida, draw it with care. Then correct the “imperfections.” The advantage of this
parting of the curtains of mere experience was clear: It provided a universal guide, one
not attached to the vagaries of individual variation.

As the sciences grew in scope, and scientists grew in number, the downside of
idealization became clearer. It was one thing to have Goethe depict the “ur-plant” or “ur-
insect.” It was quite another to have a myriad of different scientists each fixing their
images in different and sometimes contradictory ways. Gradually, from around the 1830s
forward, one begins to see something new: a claim that the image making was done with
a minimum of human intervention, that protocols were followed. This could mean
tracing a leaf with a pencil or pressing it into ink that was transferred to the page. It
meant, too, that one suddenly was proud of depicting the view through a microscope of a
natural object even with its imperfections. This was a radical idea: snowflakes shown
without perfect hexagonal symmetry, color distortion near the edge of a microscope lens,
tissue torn around the edges in the process of its preparation.

Scientific objectivity came to mean that our representations of things were
executed by holding back from intervention—even if it meant reproducing the yellow
color near the edge of the image under the microscope, despite the fact that the scientist
knew that the discoloration was from the lens, not a feature of the object of inquiry. The
advantage of objectivity was clear: It superseded the desire to see a theory realized or a
generally accepted view confirmed. But objectivity came at a cost. You lost that precise,
easily teachable, colored, full depth-of-field, artist’s rendition of a dissected corpse. You
got a blurry, bad depth-of-field, black-and-white photograph that no medical student (nor
even many medical colleagues) could use to learn and compare cases. Still, for a long
stretch of the 19th century, the virtue of hands-off, self-restraining objectivity was on the
rise.

Starting in the 1930s, the hardline scientific objectivity in scientific representation
began running into trouble. In cataloging stellar spectra, for example, no algorithm could
compete with highly trained observers who could sort them with far greater accuracy and
replicability than any purely rule-following procedure. By the late 1940s, doctors had
begun learning how to read electroencephalograms. Expert judgment was needed to sort
out different kinds of seizure readings, while none of the early attempts to use frequency
analysis could match that judgment. Solar magnetograms—mapping the magnetic fields
across the sun—required the trained expert to pry the real signal from artifacts that
emerged from the measuring instruments. Even particle physicists recognized that they
could not program a computer to sort certain kinds of tracks into the right bins; judgment,
trained judgment, was needed.

There should be no confusion here: This was not a return to the invoked genius of
an 18th-century idealizer. No one thought you could train to be a Goethe who alone
among scientists could pick out the universal, ideal form of a plant, insect, or cloud.
Expertise could be learned—you could take a course to learn to make expert judgments
about electroencephalograms, stellar spectra, or bubble-chamber tracks; alas, no one has
ever thought you could take a course that would lead to the mastery of exceptional

162

HOUSE_OVERSIGHT_016965
