332 A Glossary

e Forgetting: The process of removing Atoms from the in-RAM portion of Atomspace, when
RAM gets short and they are judged not as valuable to retain in RAM as other Atoms. This
is commonly done using the LTT values of the Atoms (removing lowest LTI-Atoms, or more
complex strategies involving the LTI of groups of interconnected Atoms). May be done by
a dedicated Forgetting MindAgent. VLTI may be used to determine the fate of forgotten
Atoms.

e Forward Chainer: A control mechanism (MindAgent) for PLN inference, that works by
taking existing Atoms and deriving conclusions from them using PLN rules, and then iter-
ating this process. The goal is to derive new Atoms that are interesting according to some
given criterion.

e Frame2Atom: A simple system of hand-coded rules for translating the output of RelEx2Frame
(logical representation of semantic relationships using FrameNet relationships) into Atoms.

e Freezing: Saving Atoms from the in-RAM Atomspace to disk.

e General Intelligence: Often used in an informal, commonsensical sense, to mean the
ability to learn and generalize beyond specific problems or contexts. Has been formalized
in various ways as well, including formalizations of the notion of ’achieving complex goals
in complex environments” and achieving complex goals in complex environments using
limited resources.” Usually interpreted as a fuzzy concept, according to which absolutely
general intelligence is physically unachievable, and humans have a significant level of general
intelligence, but far from the maximally physically achievable degree.

e Generalized Hypergraph: A hypergraph with some additional features, such as links
that point to links, and nodes that are seen as “containing” whole sub-hypergraphs. This is
the most natural and direct way to mathematically/visually model the Atomspace.

e Generator: In the PLN design, a rule is denoted a generator if it can produce its consequent
without needing premises (e.g. LookupRule, which just looks it up in the AtomSpace). See
composer.

e Global, Distributed Memory: Memory that stores items as implicit knowledge, with
each memory item spread across multiple components, stored as a pattern of organization
or activity among them.

e Glocal Memory: The storage of items in memory in a way that involves both localized
and global, distributed aspects.

e Goal: An Atom representing a function that a system (like OpenCog) is supposed to spend
a certain non-trivial percentage of its attention optimizing. The goal, informally speaking,
is to maximize the Atom’s truth value.

e Goal, Implicit: A goal that an intelligent system, in practice, strives to achieve; but that
is not explicitly represented as a goal in the system’s knowledge base.

e Goal, Explicit: A goal that an intelligent system explicitly represents in its knowledge
base, and expends some resources trying to achieve. Goal Nodes (which may be Nodes or,
e.g. ImplicationLinks) are used for this purpose in OpenCog.

e Goal-Driven Learning: Learning that is driven by the cognitive schematic i.e. by the quest
of figuring out which procedures can be expected to achieve a certain goal in a certain sort
of context.

e Grounded SchemaNode: See SchemaNode, Grounded.

e Hebbian Learning: An aspect of Attention Allocation, centered on creating and updating
HebbianLinks, which represent the simultaneous importance of the Atoms joined by the
HebbianLink.

HOUSE_OVERSIGHT_013248
