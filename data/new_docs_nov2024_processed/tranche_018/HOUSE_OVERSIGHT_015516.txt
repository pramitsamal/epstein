304 M. Hoffman et al.

50 % chance that France’s estimates are lower than its own? and, thus, that there is
a 50 % chance that France’s estimates are lower than the threshold. This further
implies that the United States assesses only a 50 % chance that France levies sanc-
tions, so the United States is not sufficiently confident that France will sanction, to
make it in the United States’s interest to sanction.

What we have shown so far is that for a threshold of 100,000, it is in the interest
of the United States to deviate from the strategy dictated by the threshold norm
when it gets a signal at the threshold. This means that 100,000 is not a viable thresh-
old, and (since 100,000 was chosen arbitrarily) there is no Nash equilibrium in
which witnesses punish if their estimate of the harm from a transgression is above
some arbitrary threshold.

It should be noted that this result only requires that there are sufficiently many
possibilities, not that there is in fact a continuum. Neither does it require that the
distribution is uniform nor that the Coordination Game is not affected by the behav-
ior of Assad. The only crucial assumptions are that the distribution is not too skewed
and that the payoffs are not too dependent on the behavior of Assad (for details, see
Dalkiran et al., 2012; Hoffman, Yoeli, & Dalkiran, 2015).

What happens if such norms are learned or evolved and subject to selection?
Suppose there is a norm to attack whenever more than 100,000 civilians are killed.
Players will soon realize that they should not attack unless, say, 100,100 civilians
are killed. Then, players will learn not to attack when they estimate 100,200 civil-
ians are killed and so on, indefinitely. Thus, every threshold will eventually
“unravel,” and no one will ever attack.®

Now let’s consider a categorical norm, for example, the use of chemical weap-
ons. We again model this as a random variable, though this time, the random vari-
able can only take on two values (0 and 1), each with some probability. Again,
players do not know with certainty whether the transgression occurred, but instead
get a noisy signal. In our example, the signal represents France or the United States’s
assessment of whether Assad used chemical weapons, and there is some likelihood
the assessors make mistakes: They might not detect chemical weapons when they
had been used or might think they have detected chemical weapons when none had
been used.

Unlike with the threshold norm, provided the likelihood of a mistaken signal is
not too high, there is a Nash equilibrium where both players punish when they
receive a signal that the transgression occurred. That is, the United States and France
each levy sanctions if their assessors detect chemical weapons. This is because
when the United States detects chemical weapons, the United States believes France

>This is where the assumption of a uniform distribution comes in. Had we instead assumed, for
instance, that the continuous variable is normally distributed, then it would not be exactly 50-50
but would deviate slightly depending on the standard deviation and the location of the threshold.
Nevertheless, the upcoming logic will still go through for most Coordination Games, i.e. any
Coordination Game with risk dominance not too close to .5.

®As with omission, this follows from iterative elimination of strictly dominated strategies (see
Hoffman et al., 2015, for details).

HOUSE_OVERSIGHT_015516
