cannot serve as the basis for Strong Al—that is, artificial intelligence that emulates
human-level reasoning and competence. To achieve human-level intelligence, learning
machines need the guidance of a blueprint of reality, a model—similar to a road map that
guides us in driving through an unfamiliar city.

To be more specific, current learning machines improve their performance by
optimizing parameters for a stream of sensory inputs received from the environment. It is
a slow process, analogous to the natural-selection process that drives Darwinian
evolution. It explains how species like eagles and snakes have developed superb vision
systems over millions of years. It cannot explain, however, the super-evolutionary
process that enabled humans to build eyeglasses and telescopes over barely a thousand
years. What humans had that other species lacked was a mental representation of their
environment—representations that they could manipulate at will to imagine alternative
hypothetical environments for planning and learning.

Historians of Homo sapiens such as Yuval Noah Harari and Steven Mithen are in
general agreement that the decisive ingredient that gave our ancestors the ability to
achieve global dominion about forty thousand years ago was their ability to create and
store a mental representation of their environment, interrogate that representation, distort
it by mental acts of imagination, and finally answer the “What if?” kind of questions.
Examples are interventional questions (“What if I do such-and-such?”) and retrospective
or counterfactual questions (“What if I had acted differently?”). No learning machine in
operation today can answer such questions. Moreover, most learning machines do not
possess a representation from which the answers to such questions can be derived.

With regard to causal reasoning, we find that you can do very little with any form
of model-blind curve fitting, or any statistical inference, no matter how sophisticated the
fitting process is. We have also found a theoretical framework for organizing such
limitations, which forms a hierarchy.

On the first level, you have statistical reasoning, which can tell you only how
seeing one event would change your belief about another. For example, what can a
symptom tell you about a disease?

Then you have a second level, which entails the first but not vice versa. It deals
with actions. “What will happen if we raise prices?” “What if you make me laugh?”
That second level of the hierarchy requires information about interventions which is not
available in the first. This information can be encoded in a graphical model, which
merely tells us which variable responds to another.

The third level of the hierarchy is the counterfactual. This is the language used by
scientists. “What if the object were twice as heavy?” “What if I were to do things
differently?” “Was it the aspirin that cured my headache, or the nap I took?”
Counterfactuals are at the top level in the sense that they cannot be derived even if we
could predict the effects of all actions. They need an extra ingredient, in the form of
equations, to tell us how variables respond to changes in other variables.

One of the crowning achievements of causal-inference research has been the
algorithmization of both interventions and counterfactuals, the top two layers of the
hierarchy. In other words, once we encode our scientific knowledge in a model (which
may be qualitative), algorithms exist that examine the model and determine if a given
query, be it about an intervention or about a counterfactual, can be estimated from the
available data—and, if so, how. This capability has transformed dramatically the way

26

HOUSE_OVERSIGHT_016246
