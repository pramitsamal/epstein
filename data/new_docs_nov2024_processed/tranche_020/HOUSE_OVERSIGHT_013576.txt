Leaving the framework of physical thermodynamic entropies entirely, the
entropy of information was introduced in the context of communication engineering
in electrical and electronic devices. The metaphorical machine for the current age of
entropy, analogous to the role of heat and steam engines in_ classical
thermodynamics, is the computer. Energy in this context is a relatively trivial
property. Ammeters and other monitors of load are unable to discriminate between
a computer actively engaged in encoding and computation or one simply
maintaining its dynamic memory while resting in computational readiness. This
situation is very analogous to the results of early work discussed previously on the
metabolic rates and sources of the whole brain’s energy, oxygen and glucose
metabolism, by National Institutes of Mental Heath’s Seymore Kety and Louis
Sokoloff and the State of Illinois Thudicum Laboratory’s Harold Himwich. Using
whole head arterial-venous, energy-in, energy-out, differences, they could not
demonstrate differences in rates of whole brain metabolism between states in which
the human subjects were engaged in solving mathematical problems or deeply
sleep. In today’s brain imaging research, using a variety of physical reflections of
the brain’s metabolic activity, it is the differences in regional distributions of
metabolic activity that are relatable to subjective and behavioral states, not
differences in total amount of energy expended. In _ graphically coded
representations of the regional metabolism of the brain in action, one or another or
many areas “light up” and others “grow dark” in correlation with changes in thinking,
feeling and action.

The entropy first developed by Claude Shannon was formalized for use in
1948 in what was then called communication theory and now information theory. It
represented a measure of the ambiguity and uncertainty that had the potential for
being resolved by new knowledge. In this context, entropy and information were
obviously complementary descriptors. A message that informs us about which of
ten possibilities should be chosen contains less information than one that informs us
about the proper choice to be made from among a thousand possibilities. The
entropy of communication theory is a measure that is computed on uncertainty. The

information reception capacity of a system is dependent upon the amount of

76

HOUSE_OVERSIGHT_013576
