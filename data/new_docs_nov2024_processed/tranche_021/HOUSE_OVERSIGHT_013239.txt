18.4 SelfModification via Theorem-Proving 323

where the Pi are predicates, and the Ni are schemata corresponding to other nodes of the
decision graph (children of the current node). Often the Pi are very simple, implementing for
instance numerical inequalities or Boolean equalities.

Once this graph has been exported into CogPrime, it can be reasoned on, used as raw material
for concept formation and predicate formation, and otherwise cognized. Supercompilation pure
and simple does not change the I/O behavior of the input program. However, the decision graph
produced during supercompilation, may be used by CogPrime cognition in order to do so. One
then has a hybrid program-modification method composed of two phases: supercompilation for
transforming programs into decision graphs, and CogPrime cognition for modifying decision
graphs so that they can have different I/O behaviors fulfilling system goals even better than
the original.

Furthermore, it seems likely that, in many cases, it may be valuable to have the super-
compiler feed many different decision-graph representations of a program into CogPrime. The
supercompiler has many internal parameters, and varying them may lead to significantly differ-
ent decision graphs. The decision graph leading to maximal optimization, may not be the one
that leads CogPrime cognition in optimal directions.

18.4 Self-Modification via Theorem-Proving

Supercompilation is a potentially very valuable tool for self-modification. If one wants to take an
existing schema and gradually improve it for speed, or even for greater effectiveness at achieving
current goals, supercompilation can potentially do that most excellently.

However, the representation that supercompilation creates for a program is very “surface-
level.” No one could read the supercompiled version of a program and understand what it
was doing. Really deep self-invented AI innovation requires, we believe, another level of self
modification beyond that provided by supercompilation. This other level, we believe, is best
formulated in terms of theorem-proving [RVO|].

Deep selfmodification could be achieved if CogPrime were capable of proving theorems of
a certain form: namely, theorems about the spacetime complexity and accuracy of particular
compound schemata, on average, assuming realistic probability distributions on the inputs, and
making appropriate independence assumptions. These are not exactly the types of theorems that
are found in human-authored mathematics papers. By and large they will be nasty, complex
theorems, not the sort that many human mathematicians enjoy proving or reading. But of
course, there is always the possibility that some elegant gem of a discovery could emerge from
this sort of highly detailed theorem-proving work.

In order to guide it in the formulation of theorems of this nature, the system will have
empirical data on the spacetime complexity of elementary schemata, and on the probability
distributions of inputs to schemata. It can embed these data in axioms, by asking: Assuming
the component elementary schemata have complexities within these bounds, and the input pdf
(probability distribution function) is between these bounds, then what is the pdf of the complexity
and accuracy of this compound schema?

Of course, this is not an easy sort of question in general: one can have schemata embodying
any sort of algorithm, including complex algorithms on which computer science professors might
write dozens of research articles. But the system must build up its ability to prove such things
incrementally, step by step.

HOUSE_OVERSIGHT_013239
