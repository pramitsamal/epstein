attending to looking. In the appendix, we formalize this intuition by showing that this
equilibrium is subgame perfect, a solution concept used to rule out these kinds of concerns
in settings where there is a high degree of rationality [15].

We might wonder whether CWOL will emerge in a population of players who are not
rational, but are evolving or learning their strategies. There are two reasons to suspect
it will not emerge. First, CWOL may be susceptible to the invasion of mutants. In
particular, the concern is that a player 2 mutant might arise that does not attend to
whether player 1 looks, rendering looking irrelevant. This mutant’s fitness is no higher or
lower than the incumbent strategy, so may grow by drift. Subsequently, a player 1 mutant
which looks and defects when the temptation is high would have a fitness advantage and
proliferate. While the player 2 mutant would then start dying off since it would now have
lower fitness than the incumbent, it is not clear that it would do so fast enough for the
player 1 mutant to also die off, returning to the CWOL equilibrium. Second, CWOL may
be stable but have such a small basin of attraction that it will never emerge. In particular,
there are three other equilibria to consider, which can be seen in figure 2. The first is
comprised of the strategy pair where player 1 always defects, and player 2 always exits
(which we refer to as the ALLD equilibrium). It is a Nash Equilibrium for all parameter
values. The second is the strategy pair where player 2 exits if player 1 defects, and player

1 cooperates with or without looking (we refer to this as the CWL equilibrium). It is a

Nash equilibrium when —— > cp. In fact, there are many more Nash Equilibria, where

1—w —
the population mixes between different strategies.

Consequently, we employ computer simulations of the replicator dynamic to explore
the evolutionary dynamics of the envelope game. The replicator dynamic is the standard

model for evolutionary dynamics [16, 17, 18, 19], and also models learning dynamics such

as reinforcement learning or prestige-biased imitation [20]. It describes strategies evolving

HOUSE_OVERSIGHT_026526
