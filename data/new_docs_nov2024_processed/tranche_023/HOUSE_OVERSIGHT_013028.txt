112 6 A Brief Overview of CogPrime

through its logic rules, so that uncertain premises give rise to conclusions with reasonably
accurately estimated uncertainty values. This careful management of uncertainty is critical for
the application of logical inference in the robotics context, where most knowledge is abstracted
from experience and is hence highly uncertain.

PLN can be used in either forward or backward chaining mode; and in the language intro-
duced above, it can be used for either analysis or synthesis. As an example, we will consider
backward chaining analysis, exemplified by the problem of a robot preschoolstudent trying to
determine whether a new playmate “Bob” is likely to be a regular visitor to is preschool or not
(evaluating the truth value of the implication Bob > regular_ visitor). The basic backward
chaining process for PLN analysis looks like:

1. Given an implication L = A — B whose truth value must be estimated (for instance
L = Concept Procedure > Goal as discussed above), create a list (Az, ..., An) of (inference
rule, stored knowledge) pairs that might be used to produce L

2. Using analogical reasoning to prior inferences, assign each A; a probability of success

e If some of the A; are estimated to have reasonable probability of success at generating
reasonably confident estimates of D’s truth value, then invoke Step 1 with A; in place
of LE (at this point the inference process becomes recursive)

e If none of the A; looks sufficiently likely to succeed, then inference has “gotten stuck”
and another cognitive process should be invoked, e.g.

— Concept creation may be used to infer new concepts related to A and B, and then
Step 1 may be revisited, in the hope of finding a new, more promising A; involving
one of the new concepts

— MOSES may be invoked with one of several special goals, e.g. the goal of finding
a procedure P so that P(X) predicts whether X — B. If MOSES finds such a
procedure P then this can be converted to declarative knowledge understandable
by PLN and Step 1 may be revisited....

— Simulations may be run in CogPrime’s internal simulation engine, so as to observe
the truth value of A > B in the simulations; and then Step 1 may be revisited...

The combinatorial explosion of inference control is combatted by the capability to defer to
other cognitive processes when the inference control procedure is unable to make a sufficiently
confident choice of which inference steps to take next. Note that just as MOSES may rely
on PLN to model its evolving populations of procedures, PLN may rely on MOSES to create
complex knowledge about the terms in its logical implications. This is just one example of the
multiple ways in which the different cognitive processes in CogPrime interact synergetically; a
more thorough treatment of these interactions is given in [Goe09al.

In the “new playmate” example, the interesting case is where the robot initially seems not
to know enough about Bob to make a solid inferential judgment (so that none of the A; seem
particularly promising). For instance, it might carry out a number of possible inferences and not
come to any reasonably confident conclusion, so that the reason none of the A; seem promising
is that all the decent-looking ones have been tried already. So it might then recourse to MOSES,
simulation or concept creation.

For instance, the PLN controller could make a list of everyone who has been a regular
visitor, and everyone who has not been, and pose MOSES the task of figuring out a procedure
for distinguishing these two categories. This procedure could then be used directly to make the
needed assessment, or else be translated into logical rules to be used within PLN inference. For

HOUSE_OVERSIGHT_013028
