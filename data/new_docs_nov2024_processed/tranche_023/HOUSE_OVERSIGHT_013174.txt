258 13 Local, Global and Glocal Knowledge Representation

13.5 Neural Foundations of Learning

Now we move from knowledge representation to learning — which is after all nothing but the
adaptation of represented knowledge based on stimulus, reinforcement and spontaneous activity.
While our focus in this chapter is on representation, it’s not possible for us to make our points
about glocal knowledge representation in neural net type systems without discussing some
aspects of learning in these systems.

13.5.1 Hebbian Learning

The most common and plausible assumption about learning in the brain is that synaptic connec-
tions between neurons are adapted via some variant of Hebbian learning. The original Hebbian
learning rule, proposed by Donald Hebb in his 1949 book [Heb49], was roughly

1. The weight of the synapse 2 > y increases if x and y fire at roughly the same time
2. The weight of the synapse « — y decreases if x fires at a certain time but y does not

Over the years since Hebb’s original proposal, many neurobiologists have sought evidence that
the brain actually uses such a method. One of the things they have found, so far, is a lot of
evidence for the following learning rule [DC02, LS05]:

1. The weight of the synapse x > y increases if x fires shortly before y does
2. The weight of the synapse « — y decreases if x fires shortly after y does

The new thing here, not foreseen by Donald Hebb, is the “postsynaptic depression” involved in
rule component 2.

Now, the simple rule stated above does not sum up all the research recently done on Hebbian-
type learning mechanisms in the brain. The real biological story underlying these approximate
rules is quite complex, involving many particulars to do with various neurotransmitters. IIl-
understood details aside, however, there is an increasing body of evidence that not only does
this sort of learning occur in the brain, but it leads to distributed experience-based neural
modification: that is, one instance synaptic modification causes another instance of synaptic
modification, which causes another, and so forth? [Bi01].

13.5.2 Virtual Synapses and Hebbian Learning Between Assemblies

Hebbian learning is conventionally formulated in terms of individual neurons, but, it can be
extended naturally to assemblies via defining “virtual synapses” between assemblies.

Since assemblies are sets of neurons, one can view a synapse as linking two assemblies
if it links two neurons, each of which is in one of the assemblies. One can then view
two assemblies as being linked by a bundle of synapses. We can define the weight of the
synaptic bundle from assembly Al to assembly A2 as the number w so that (the change

? This has been observed in “model systems” consisting of neurons extracted from a brain and hooked together
in a laboratory setting and monitored; measurement of such dynamics in vivo is obviously more difficult.

HOUSE_OVERSIGHT_013174
