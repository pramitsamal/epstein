or are too “high frequency” (like static) rather than “low frequency” (more
continuous, like actual real-world features).

Just because AI systems sometimes end up in local minima, don’t conclude that this
makes them any less like life. Humans—indeed, probably all life-forms—are often stuck
in local minima.

Take our understanding of the game of Go, which was taught and learned and
optimized by humans for thousands of years. It took Als less than three years to find out
that we’d been playing it wrong all along and that there were better, almost alien,
solutions to the game which we’d never considered—mostly because our brains don’t
have the processing power to consider so many moves ahead.

Even in chess, which is ten times easier and was thought to be understood, brute-
force machines could beat us at our own strategies. Chess, too, turned out, when
explored by superior neural-network AI systems, to have weird but superior strategies
we'd never considered, like sacrificing queens early to gain an obscure long-term
advantage. It’s as if we had been playing 2D versions of games that actually existed in
higher dimensions.

If any of this sounds familiar, it’s because physics has been wrestling with these
sorts of topological problems for decades. The notion of space being many-dimensional,
and math reducing to understanding the geometries and interactions of “membranes”
beyond the reach of our senses, is where Grand Unified Theorists go to die. But unlike
multidimensional theoretical physics, AI is something we can actually experiment with
and measure.

So that’s what we’re going to do. The next few decades will be an explosive
exploration of ways to think that 7 million years of evolution never found. We’re going
to rock ourselves out of local minima and find deeper minima, maybe even global
minima. And when we’re done, we may even have taught machines to seem as smart as
a mosquito, forever descending the cosmic gradients to an ultimate goal, whatever that
may be.

107

HOUSE_OVERSIGHT_016910
