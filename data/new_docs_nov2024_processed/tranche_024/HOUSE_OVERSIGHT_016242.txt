only until it runs out of fuel. Similarly, the exponential advances in Moore’s Law are
starting to run into limits imposed by basic physics. The clock speed of computers
maxed out at a few gigahertz a decade and a half ago, simply because the chips were
starting to melt. The miniaturization of transistors is already running into quantum-
mechanical problems due to tunneling and leakage currents. Eventually, the various
exponential improvements in memory and processing driven by Moore’s Law will
grind to a halt. A few more decades, however, will probably be time enough for the
raw information-processing power of computers to match that of brains—at least by
the crude measures of number of bits and number of bit-flips per second.

Human brains are intricately constructed, the process of millions of years of
natural selection. In Wiener’s time, our understanding of the architecture of the brain
was rudimentary and simplistic. Since then, increasingly sensitive instrumentation and
imaging techniques have shown our brains to be far more varied in structure and
complex in function than Wiener could have imagined. I recently asked Tomaso
Poggio, one of the pioneers of modern neuroscience, whether he was worried that
computers, with their rapidly increasing processing power, would soon emulate the
functioning of the human brain. “Not a chance,” he replied.

The recent advances in deep learning and neuromorphic computation are very
good at reproducing a particular aspect of human intelligence focused on the operation
of the brain’s cortex, where patterns are processed and recognized. These advances
have enabled a computer to beat the world champion not just of chess but of Go, an
impressive feat, but they’re far short of enabling a computerized robot to tidy a room.
(In fact, robots with anything approaching human capability in a broad range of
flexible movements are still far away—search “robots falling down.” Robots are good
at making precision welds on assembly lines, but they still can’t tie their own shoes.)

Raw information-processing power does not mean sophisticated information-
processing power. While computer power has advanced exponentially, the programs
by which computers operate have often failed to advance at all. One of the primary
responses of software companies to increased processing power is to add “useful”
features which often make the software harder to use. Microsoft Word reached its
apex in 1995 and has been slowly sinking under the weight of added features ever
since. Once Moore’s Law starts slowing down, software developers will be confronted
with hard choices between efficiency, speed, and functionality.

A major fear of the singulariteers is that as computers become more involved in
designing their own software they’! rapidly bootstrap themselves into achieving
superhuman computational ability. But the evidence of machine learning points in the
opposite direction. As machines become more powerful and capable of learning, they
learn more and more as human beings do—from multiple examples, often under the
supervision of human and machine teachers. Education is as hard and slow for
computers as it is for teenagers. Consequently, systems based on deep learning are
becoming more rather than less human. The skills they bring to learning are not
“better than” but “complementary to” human learning: Computer learning systems can
identify patterns that humans cannot—and vice versa. The world’s best chess players
are neither computers nor humans but humans working together with computers.
Cyberspace is indeed inhabited by harmful programs, but these primarily take the form
of malware—viruses notable for their malign mindlessness, not for their

22

HOUSE_OVERSIGHT_016242
